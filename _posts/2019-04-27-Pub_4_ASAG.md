xx---
layout: post
title:  "Surprisingly Simple Short Answer Grading"
date:   2019-04-27
excerpt: "Automatic short answer grading given a question, student answer and reference answer."
project: false
publication: true
tag:
- ASAG
- AIED
- Extratrees
feature: /assets/img/sky1.jpg
comments: true
---
#### Conference 
 The 20th International Conference on Artificial Intelligence in Education (AIED).
 
 

#### Abstract 
Automatic Short Answer Grading(ASAG) is an important area in the application of natural language process-ing to the education sector. Human evaluators may lack consistency in grading due to bias, fatigue, or otherfactors. According to Sukkareih et al, manual evaluations can take up to twelve hours per question. More-over, the growing student population and the different types of assessment practices demand an automatic,efficient and cost-effective grading system. Answer grading systems have been split into many categoriesover the years based on several factors, the most fundamental one being the type of question. Short AnswerGrading Systems, in specific, usually satisfy five broad criteria mentioned in Burrows et al. Firstly, they require external knowledge apart from just the reference answer. Second, the response answers should bein natural language. Third, the answer length should be between one phrase and one paragraph. Fourth, theassessment would focus on the content and not the writing style. Lastly, the openness of the responses shouldbe restricted by the question design.
Keeping in mind the nature, size and complexity of data being dealt with and the immediate requirementof grading results in ASAG system, the need for a basic model along with handcrafted features becomescrucial. In this paper, we propose a set of simple features which beat most of the current state-of-the-artresults using tree-based classifiers and regressors. These features are straightforward and require minimalcomputation and resources, making them ideal for Short Answer Grading. We first discuss the differentphases of feature engineering followed by our experimentation on two benchmark datasets. We tabulatephase-wise results to understand the importance of features added at every phase and finally compare ourperformances with the existing state-of-the-art approaches.
